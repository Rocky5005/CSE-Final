{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import file_locate\n",
    "from ml_models import logistic_regression, naive_bayes, k_nearest_neighbors\n",
    "from ml_models import support_vector, gradient_boost, random_forest\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(labels_test, labels_pred):\n",
    "    cm = confusion_matrix(labels_test, labels_pred)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "    cm_display.plot()\n",
    "    plt.show()\n",
    "    accuracy = accuracy_score(labels_test, labels_pred)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    recall= recall_score(labels_test, labels_pred)\n",
    "    print(\"Recall: \", recall)\n",
    "    f1 = f1_score(labels_test, labels_pred)\n",
    "    print(\"F1: \", f1)\n",
    "    report = classification_report(labels_test, labels_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(labels_test, labels_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(labels_test, labels_pred)\n",
    "    auc = roc_auc_score(labels_test, labels_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(labels_test, labels_pred):\n",
    "    precision, recall, _ = precision_recall_curve(labels_test, labels_pred)\n",
    "    plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(labels_test, labels_pred):\n",
    "    statistics(labels_test, labels_pred)\n",
    "    auc(labels_test, labels_pred)\n",
    "    precision_recall(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_locate('ml-analysis.ipynb')\n",
    "filename = 'cleaned-framingham.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "logistic_regression.hyperparameter_search(filename, smote_scaled)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "logistic_regression.hyperparameter_search(filename, sampled_scaled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "scaled = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, smote_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression(C=0.1, max_iter=50, penalty='l1', solver='liblinear'))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, hyperparameter_sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', LogisticRegression(C=0.01, max_iter=50, penalty='l2', solver='liblinear'))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = logistic_regression.logistic_regression(filename, hyperparameter_smote_scaled)\n",
    "analysis(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', ComplementNB())\n",
    "    ])\n",
    "naive_bayes.hyperparameter_search(filename, smote)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', ComplementNB())\n",
    "    ])\n",
    "naive_bayes.hyperparameter_search(filename, sampled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', ComplementNB())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = naive_bayes.naive_bayes(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', ComplementNB())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = naive_bayes.naive_bayes(filename, sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', ComplementNB())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = naive_bayes.naive_bayes(filename, smote)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', ComplementNB(alpha=0.5, fit_prior=True, norm=False))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = naive_bayes.naive_bayes(filename, hyperparameter_sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', ComplementNB(alpha=0.1, fit_prior=True, norm=False))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = naive_bayes.naive_bayes(filename, hyperparameter_smote)\n",
    "analysis(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "# k_nearest_neighbors.hyperparameter_search(filename, smote_scaled)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "# k_nearest_neighbors.hyperparameter_search(filename, sampled_scaled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "scaled = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, smote_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier(algorithm='brute', metric='manhattan', n_neighbors=13, weights='uniform'))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, hyperparameter_sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', KNeighborsClassifier(algorithm='brute', metric='manhattan', n_neighbors=11, weights='distance'))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = k_nearest_neighbors.k_nearest(filename, hyperparameter_smote_scaled)\n",
    "analysis(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "# support_vector.hyperparameter_search(filename, smote_scaled)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "# support_vector.hyperparameter_search(filename, sampled_scaled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "scaled = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, smote_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC(C=1000, class_weight='balanced', gamma='scale', kernel='linear', shrinking=False))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, hyperparameter_sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', SVC(C=0.001, class_weight=None, gamma='scale', kernel='linear', shrinking=False))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = support_vector.support_vector(filename, hyperparameter_smote_scaled)\n",
    "analysis(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier())\n",
    "    ])\n",
    "gradient_boost.hyperparameter_search(filename, smote)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier())\n",
    "    ])\n",
    "gradient_boost.hyperparameter_search(filename, sampled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', xgb.XGBClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = gradient_boost.gradient_boost(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = gradient_boost.gradient_boost(filename, sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = gradient_boost.gradient_boost(filename, smote)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier(booster='gblinear', learning_rate=0.1, n_estimators=1000))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = gradient_boost.gradient_boost(filename, hyperparameter_sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', xgb.XGBClassifier(booster='gblinear', learning_rate=0.1, n_estimators=1000))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = gradient_boost.gradient_boost(filename, hyperparameter_smote)\n",
    "analysis(labels_test, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "random_forest.hyperparameter_search(filename, smote)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "random_forest.hyperparameter_search(filename, sampled)\n",
    "\n",
    "base = Pipeline([\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, base)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, smote)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "sampled_scaled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, sampled_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "smote_scaled = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('classification', RandomForestClassifier())\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, smote_scaled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_sampled = Pipeline([\n",
    "        ('sampling', RandomOverSampler(random_state=42)),\n",
    "        ('classification', RandomForestClassifier(criterion='entropy', max_depth=5, max_features='sqrt', n_estimators=200))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, hyperparameter_sampled)\n",
    "analysis(labels_test, labels_pred)\n",
    "\n",
    "hyperparameter_smote = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('classification', RandomForestClassifier(criterion='entropy', max_depth=5, max_features='sqrt', n_estimators=300))\n",
    "    ])\n",
    "labels_train, labels_train_pred, labels_test, labels_pred = random_forest.random_forest(filename, hyperparameter_smote)\n",
    "analysis(labels_test, labels_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
